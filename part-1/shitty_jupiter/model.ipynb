{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34fdd00",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "Building a linear regression model to predict total bike rentals (`cnt`) using weather features: temperature, humidity, and windspeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df6ce1",
   "metadata": {},
   "source": [
    "### Load dependencies and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd474d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# load data\n",
    "data_dir = Path.cwd()\n",
    "\n",
    "day_df = pd.read_csv(data_dir / 'day.csv', parse_dates=['dteday'])\n",
    "hour_df = pd.read_csv(data_dir / 'hour.csv', parse_dates=['dteday'])\n",
    "\n",
    "# make a normal timeline 2011-01-01 08:00:00\n",
    "hour_df['datetime'] = hour_df['dteday'] + pd.to_timedelta(hour_df['hr'], unit='h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the available categorical features in the dataset\n",
    "print(\"Hour DataFrame Info:\")\n",
    "print(hour_df.info())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(hour_df.head())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nCategorical columns and their unique values:\")\n",
    "for col in ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']:\n",
    "    if col in hour_df.columns:\n",
    "        print(f\"\\n{col}: {sorted(hour_df[col].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9fefd",
   "metadata": {},
   "source": [
    "### Understanding Categorical Encoding\n",
    "\n",
    "**Good news!** Your dataset already has categorical features encoded as integers:\n",
    "\n",
    "- **season**: 1=Winter, 2=Spring, 3=Summer, 4=Fall\n",
    "- **yr**: 0=2011, 1=2012\n",
    "- **mnth**: 1-12 (January to December)\n",
    "- **hr**: 0-23 (hour of day)\n",
    "- **weekday**: 0-6 (day of week)\n",
    "- **weathersit**: 1-4 (weather conditions)\n",
    "- **holiday**, **workingday**: 0=No, 1=Yes\n",
    "\n",
    "**Do you need to encode them further?**\n",
    "\n",
    "It depends on the type of categorical variable:\n",
    "\n",
    "1. **Ordinal variables** (have a natural order): Can use as-is\n",
    "   - Example: `weathersit` (1=Clear ‚Üí 4=Heavy Rain/Snow) - worse weather has higher values\n",
    "   \n",
    "2. **Nominal variables** (no natural order): Should use **One-Hot Encoding**\n",
    "   - Example: `season` - Spring isn't \"greater than\" Winter, they're just different\n",
    "   - Example: `weekday` - Monday isn't \"less than\" Friday\n",
    "\n",
    "**When to use One-Hot Encoding:**\n",
    "- Linear models can misinterpret ordinal encoding as having magnitude\n",
    "- If `season=4` (Fall), the model might think it's \"4 times more\" than `season=1` (Winter)\n",
    "- One-Hot creates binary columns: `season_1`, `season_2`, `season_3`, `season_4`\n",
    "\n",
    "Let's explore both approaches below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d39fdd2",
   "metadata": {},
   "source": [
    "### Approach 1: Using Features Directly (Current Method)\n",
    "Simply use the already-encoded integers. This works but can mislead the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding categorical features directly\n",
    "features_direct = ['temp', 'hum', 'windspeed', 'season', 'hr', 'weekday']\n",
    "\n",
    "X_direct = hour_df[features_direct]\n",
    "print(\"Shape with direct encoding:\", X_direct.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(X_direct.head())\n",
    "print(\"\\n‚ö†Ô∏è Problem: The model thinks season=4 is '4 times' season=1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d4463c",
   "metadata": {},
   "source": [
    "### Approach 2: One-Hot Encoding with pd.get_dummies()\n",
    "Creates separate binary columns for each category - the proper way for linear models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceeced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas get_dummies for one-hot encoding\n",
    "print(\"=\"*80)\n",
    "print(\"ONE-HOT ENCODING WITH pd.get_dummies()\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select continuous and categorical features\n",
    "continuous_features = ['temp', 'hum', 'windspeed']\n",
    "categorical_features = ['season', 'hr', 'weekday']\n",
    "\n",
    "# Get continuous features\n",
    "X_continuous = hour_df[continuous_features]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X_categorical = pd.get_dummies(hour_df[categorical_features], \n",
    "                                columns=categorical_features,\n",
    "                                drop_first=True,  # Avoid multicollinearity\n",
    "                                prefix=['season', 'hr', 'weekday'])\n",
    "\n",
    "# Combine them\n",
    "X_encoded = pd.concat([X_continuous, X_categorical], axis=1)\n",
    "\n",
    "print(f\"\\nOriginal shape: {hour_df[continuous_features + categorical_features].shape}\")\n",
    "print(f\"After one-hot encoding: {X_encoded.shape}\")\n",
    "print(f\"\\nNew columns created: {X_encoded.shape[1] - len(continuous_features)} binary features\")\n",
    "print(f\"\\nColumn names (first 20):\")\n",
    "print(X_encoded.columns.tolist()[:20])\n",
    "\n",
    "print(\"\\n‚úÖ Each category now has its own binary column!\")\n",
    "print(\"‚úÖ No more false magnitude relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134fa15b",
   "metadata": {},
   "source": [
    "### üéØ Summary: Direct Encoding vs One-Hot Encoding\n",
    "\n",
    "| Method | When to Use | Pros | Cons |\n",
    "|--------|-------------|------|------|\n",
    "| **Direct (integers)** | Ordinal data with meaningful order | Simple, fewer features | Can mislead linear models |\n",
    "| **pd.get_dummies()** | Most categorical features | Prevents false relationships | More features created |\n",
    "\n",
    "**Key Rule:** For **linear models** (like LinearRegression), **always use one-hot encoding** with `pd.get_dummies()` for nominal categorical variables (season, day of week, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92da573",
   "metadata": {},
   "source": [
    "### üìä Practical Example: Model with get_dummies() Encoding\n",
    "Let's rebuild your model with proper one-hot encoding using `pd.get_dummies()` and compare results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c37656",
   "metadata": {},
   "source": [
    "### üí° Key Takeaways: Using get_dummies() for One-Hot Encoding\n",
    "\n",
    "**How to use get_dummies() for categorical encoding:**\n",
    "\n",
    "```python\n",
    "# Select your features\n",
    "continuous_features = ['temp', 'hum', 'windspeed']\n",
    "categorical_features = ['season', 'hr', 'weekday', 'weathersit']\n",
    "\n",
    "# Combine all features\n",
    "all_features = df[continuous_features + categorical_features]\n",
    "\n",
    "# Apply one-hot encoding\n",
    "X_encoded = pd.get_dummies(all_features, \n",
    "                           columns=categorical_features, \n",
    "                           drop_first=True)\n",
    "```\n",
    "\n",
    "**Why `drop_first=True`?**\n",
    "- Prevents multicollinearity in linear regression\n",
    "- If season_2=0, season_3=0, season_4=0, then it must be season_1!\n",
    "- Required for linear models to work properly\n",
    "\n",
    "**When to use this approach:**\n",
    "- ‚úÖ Your data has categorical variables encoded as integers (season: 1-4, hr: 0-23, etc.)\n",
    "- ‚úÖ You're using linear models (these assume linear relationships)\n",
    "- ‚úÖ You want to tell the model: \"these are categories, not magnitudes\"\n",
    "- ‚úÖ Perfect for Jupyter notebooks and data exploration\n",
    "\n",
    "**Results:**\n",
    "- Transforms nominal categories into binary columns\n",
    "- Model performance improves dramatically (R¬≤: 0.26 ‚Üí 0.62)\n",
    "- Simple, clean pandas code - no complex pipelines needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train improved model with get_dummies\n",
    "print(\"Training model with pd.get_dummies() encoding...\")\n",
    "\n",
    "# Select features\n",
    "continuous_cols = ['temp', 'hum', 'windspeed']\n",
    "categorical_cols = ['season', 'hr', 'weekday', 'weathersit']\n",
    "\n",
    "# Create dataframe with all features\n",
    "all_features = hour_df[continuous_cols + categorical_cols].copy()\n",
    "\n",
    "# Apply get_dummies - this will expand categorical columns into binary features\n",
    "X_improved = pd.get_dummies(all_features, columns=categorical_cols, drop_first=True)\n",
    "y_improved = hour_df['cnt']\n",
    "\n",
    "# Split data with same random_state for fair comparison\n",
    "X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(\n",
    "    X_improved, y_improved, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model_improved = LinearRegression()\n",
    "model_improved.fit(X_train_imp, y_train_imp)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_imp = model_improved.predict(X_train_imp)\n",
    "y_test_pred_imp = model_improved.predict(X_test_imp)\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2_improved = r2_score(y_train_imp, y_train_pred_imp)\n",
    "train_rmse_improved = np.sqrt(mean_squared_error(y_train_imp, y_train_pred_imp))\n",
    "train_mae_improved = mean_absolute_error(y_train_imp, y_train_pred_imp)\n",
    "\n",
    "r2_improved = r2_score(y_test_imp, y_test_pred_imp)\n",
    "rmse_improved = np.sqrt(mean_squared_error(y_test_imp, y_test_pred_imp))\n",
    "mae_improved = mean_absolute_error(y_test_imp, y_test_pred_imp)\n",
    "\n",
    "print(f\"‚úÖ Model with get_dummies trained! ({X_improved.shape[1]} features)\\n\")\n",
    "\n",
    "# Visual comparison of encoding impact (2 models)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original model\n",
    "axes[0].bar(['R¬≤', 'RMSE\\n(√∑100)', 'MAE\\n(√∑100)'], \n",
    "            [test_r2, test_rmse/100, test_mae/100],\n",
    "            color=['#3498db', '#e74c3c', '#f39c12'],\n",
    "            edgecolor='black',\n",
    "            linewidth=2)\n",
    "axes[0].set_title('Original Model\\n(Only Weather Features)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Metric Value', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim(0, max(test_rmse/100, test_mae/100) * 1.2)\n",
    "\n",
    "# Model with get_dummies\n",
    "axes[1].bar(['R¬≤', 'RMSE\\n(√∑100)', 'MAE\\n(√∑100)'], \n",
    "            [r2_improved, rmse_improved/100, mae_improved/100],\n",
    "            color=['#2ecc71', '#e74c3c', '#f39c12'],\n",
    "            edgecolor='black',\n",
    "            linewidth=2)\n",
    "axes[1].set_title('With pd.get_dummies()\\n(One-Hot Encoded Features)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Metric Value', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].set_ylim(0, max(test_rmse/100, test_mae/100) * 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä MODEL COMPARISON: Before & After One-Hot Encoding\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Metric':<15} {'Original':<20} {'With get_dummies()':<20} {'Change':<15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'R¬≤ Score':<15} {test_r2:<20.4f} {r2_improved:<20.4f} {(r2_improved-test_r2):<+15.4f}\")\n",
    "print(f\"{'RMSE':<15} {test_rmse:<20.2f} {rmse_improved:<20.2f} {(rmse_improved-test_rmse):<+15.2f}\")\n",
    "print(f\"{'MAE':<15} {test_mae:<20.2f} {mae_improved:<20.2f} {(mae_improved-test_mae):<+15.2f}\")\n",
    "print(f\"{'Features':<15} {X_train.shape[1]:<20} {X_improved.shape[1]:<20} {X_improved.shape[1] - X_train.shape[1]:<+15}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüí° Key Takeaways:\")\n",
    "print(f\"   ‚úÖ One-hot encoding with get_dummies() dramatically improves model performance\")\n",
    "print(f\"   ‚úÖ R¬≤ improved from {test_r2:.2f} ‚Üí {r2_improved:.2f} (+{((r2_improved-test_r2)/test_r2)*100:.0f}%!)\")\n",
    "print(f\"   ‚úÖ Model now captures temporal patterns (hour, day, season)\")\n",
    "print(f\"   ‚úÖ Simple to use: just pd.get_dummies(df, columns=cols, drop_first=True)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f522f790",
   "metadata": {},
   "source": [
    "### Setup MLflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cdad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(\"bike-sharing-prediction\")\n",
    "\n",
    "# Set tracking URI to local directory\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "\n",
    "print(\"MLflow experiment setup complete!\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment Name: bike-sharing-prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c021faa2",
   "metadata": {},
   "source": [
    "### Prepare data for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = ['temp', 'hum', 'windspeed']\n",
    "target = 'cnt'\n",
    "\n",
    "# Prepare X (features) and y (target)\n",
    "X = hour_df[features]\n",
    "y = hour_df[target]\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Testing set size: {len(X_test)} samples\")\n",
    "print(f\"\\nFeatures used: {features}\")\n",
    "print(f\"Target variable: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb37b5",
   "metadata": {},
   "source": [
    "### Train the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b73aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"linear_regression_weather_features\"):\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "     \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    \n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"features\", features)\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    \n",
    "    # Log model coefficients\n",
    "    for feature, coef in zip(features, model.coef_):\n",
    "        mlflow.log_param(f\"coef_{feature}\", round(coef, 2))\n",
    "    mlflow.log_param(\"intercept\", round(model.intercept_, 2))\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_r2\", train_r2)\n",
    "    mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "    mlflow.log_metric(\"train_mae\", train_mae)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(model, \"linear_regression_model\")\n",
    "    \n",
    "    print(\"Model trained successfully!\")\n",
    "    print(f\"\\nModel Coefficients:\")\n",
    "    for feature, coef in zip(features, model.coef_):\n",
    "        print(f\"  {feature}: {coef:.2f}\")\n",
    "    print(f\"\\nIntercept: {model.intercept_:.2f}\")\n",
    "    print(\"\\n‚úì All parameters, metrics, and model logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790751d",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTraining Set:\")\n",
    "print(f\"  R¬≤ Score:  {train_r2:.4f}\")\n",
    "print(f\"  RMSE:      {train_rmse:.2f}\")\n",
    "print(f\"  MAE:       {train_mae:.2f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  R¬≤ Score:  {test_r2:.4f}\")\n",
    "print(f\"  RMSE:      {test_rmse:.2f}\")\n",
    "print(f\"  MAE:       {test_mae:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"The model explains {test_r2*100:.2f}% of the variance in bike rentals\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78131223",
   "metadata": {},
   "source": [
    "### Visualize predictions vs actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of actual vs predicted values using seaborn\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "sns.scatterplot(x=y_train, y=y_train_pred, alpha=0.6, edgecolor='black', linewidth=0.5, ax=axes[0])\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Rentals', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Predicted Rentals', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Training Set (R¬≤ = {train_r2:.4f})', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Test set\n",
    "sns.scatterplot(x=y_test, y=y_test_pred, alpha=0.6, color='orange', edgecolor='black', linewidth=0.5, ax=axes[1])\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Rentals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Predicted Rentals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'Test Set (R¬≤ = {test_r2:.4f})', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d8afe4",
   "metadata": {},
   "source": [
    "### Residual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87093a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Create residual plots using seaborn\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "sns.scatterplot(x=y_test_pred, y=residuals, alpha=0.6, edgecolor='black', linewidth=0.5, ax=axes[0])\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Rentals', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Residuals', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Residuals vs Predicted Values', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Histogram of residuals\n",
    "sns.histplot(residuals, bins=30, kde=True, edgecolor='black', alpha=0.7, ax=axes[1])\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Distribution of Residuals', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85778046",
   "metadata": {},
   "source": [
    "### View MLflow Experiment Results\n",
    "Query and display the logged experiments and runs from MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc2c8acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: bike-sharing-prediction\n",
      "Experiment ID: 959130250668203271\n",
      "\n",
      "Recent Runs (5 found):\n",
      "================================================================================\n",
      "\n",
      "Run #1:\n",
      "  Run ID: 51dd323094e040a68d4002e270721b76\n",
      "  Run Name: linear_regression_weather_features\n",
      "  Status: FINISHED\n",
      "  Start Time: 2025-11-04 15:38:39.957000\n",
      "\n",
      "  Metrics:\n",
      "    test_mae: 115.1557\n",
      "    test_r2: 0.2562\n",
      "    test_rmse: 153.4726\n",
      "    train_mae: 117.9558\n",
      "    train_r2: 0.2500\n",
      "    train_rmse: 157.8005\n",
      "\n",
      "  Parameters:\n",
      "    coef_hum: -275.93\n",
      "    coef_temp: 360.56\n",
      "    coef_windspeed: 19.84\n",
      "    intercept: 180.48\n",
      "\n",
      "Run #2:\n",
      "  Run ID: c9da1b1ecc5e4c34a88d4fdcff605457\n",
      "  Run Name: linear_regression_weather_features\n",
      "  Status: FINISHED\n",
      "  Start Time: 2025-11-04 15:34:44.017000\n",
      "\n",
      "  Metrics:\n",
      "    test_mae: 115.1557\n",
      "    test_r2: 0.2562\n",
      "    test_rmse: 153.4726\n",
      "    train_mae: 117.9558\n",
      "    train_r2: 0.2500\n",
      "    train_rmse: 157.8005\n",
      "\n",
      "  Parameters:\n",
      "    coef_hum: -275.93\n",
      "    coef_temp: 360.56\n",
      "    coef_windspeed: 19.84\n",
      "    intercept: 180.48\n",
      "\n",
      "Run #3:\n",
      "  Run ID: 6a7e8b7ab5e74f7c87210c541aaefc22\n",
      "  Run Name: linear_regression_weather_features\n",
      "  Status: FINISHED\n",
      "  Start Time: 2025-11-04 15:15:51.731000\n",
      "\n",
      "  Metrics:\n",
      "    test_mae: 115.1557\n",
      "    test_r2: 0.2562\n",
      "    test_rmse: 153.4726\n",
      "    train_mae: 117.9558\n",
      "    train_r2: 0.2500\n",
      "    train_rmse: 157.8005\n",
      "\n",
      "  Parameters:\n",
      "    coef_hum: -275.93\n",
      "    coef_temp: 360.56\n",
      "    coef_windspeed: 19.84\n",
      "    intercept: 180.48\n",
      "\n",
      "Run #4:\n",
      "  Run ID: a1aaa203bd0046f5a411e834103e08eb\n",
      "  Run Name: linear_regression_weather_features\n",
      "  Status: FINISHED\n",
      "  Start Time: 2025-11-04 15:06:28.151000\n",
      "\n",
      "  Metrics:\n",
      "    test_mae: 115.1557\n",
      "    test_r2: 0.2562\n",
      "    test_rmse: 153.4726\n",
      "    train_mae: 117.9558\n",
      "    train_r2: 0.2500\n",
      "    train_rmse: 157.8005\n",
      "\n",
      "  Parameters:\n",
      "    coef_hum: -275.93\n",
      "    coef_temp: 360.56\n",
      "    coef_windspeed: 19.84\n",
      "    intercept: 180.48\n",
      "\n",
      "Run #5:\n",
      "  Run ID: b5ca9e4c87244528b4dce7df55beb9e2\n",
      "  Run Name: linear_regression_weather_features\n",
      "  Status: FINISHED\n",
      "  Start Time: 2025-11-04 15:05:42.846000\n",
      "\n",
      "  Metrics:\n",
      "    test_mae: 115.1557\n",
      "    test_r2: 0.2562\n",
      "    test_rmse: 153.4726\n",
      "    train_mae: 117.9558\n",
      "    train_r2: 0.2500\n",
      "    train_rmse: 157.8005\n",
      "\n",
      "  Parameters:\n",
      "    coef_hum: -275.93\n",
      "    coef_temp: 360.56\n",
      "    coef_windspeed: 19.84\n",
      "    intercept: 180.48\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìä To view the MLflow UI, run this command in terminal:\n",
      "   mlflow ui --backend-store-uri file:./mlruns\n",
      "\n",
      "Then open: http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "# Search for runs in the experiment\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(\"bike-sharing-prediction\")\n",
    "\n",
    "if experiment:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=5\n",
    "    )\n",
    "    \n",
    "    print(f\"Experiment: {experiment.name}\")\n",
    "    print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "    print(f\"\\nRecent Runs ({len(runs)} found):\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, run in enumerate(runs, 1):\n",
    "        print(f\"\\nRun #{i}:\")\n",
    "        print(f\"  Run ID: {run.info.run_id}\")\n",
    "        print(f\"  Run Name: {run.data.tags.get('mlflow.runName', 'N/A')}\")\n",
    "        print(f\"  Status: {run.info.status}\")\n",
    "        print(f\"  Start Time: {pd.to_datetime(run.info.start_time, unit='ms')}\")\n",
    "        \n",
    "        print(f\"\\n  Metrics:\")\n",
    "        for metric, value in sorted(run.data.metrics.items()):\n",
    "            print(f\"    {metric}: {value:.4f}\")\n",
    "        \n",
    "        print(f\"\\n  Parameters:\")\n",
    "        for param, value in sorted(run.data.params.items()):\n",
    "            if param.startswith('coef_') or param == 'intercept':\n",
    "                print(f\"    {param}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nüìä To view the MLflow UI, run this command in terminal:\")\n",
    "    print(\"   mlflow ui --backend-store-uri file:./mlruns\")\n",
    "    print(\"\\nThen open: http://localhost:5000\")\n",
    "else:\n",
    "    print(\"No experiment found. Please run the model training cell first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
